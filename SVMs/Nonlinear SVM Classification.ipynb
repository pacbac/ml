{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear SVMs using the LinearSVC class\n",
    "(doesn't know the kernel trick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Split data up. The moons dataset consists of an instance [feature1, feature2] matched with the correct class [0/1/2]\n",
    "\n",
    "validation_ratio = 0.25\n",
    "validation_length = int(validation_ratio * len(moons[0]))\n",
    "train_length = int((1 - validation_ratio) * len(moons[0]))\n",
    "X_train = moons[0][:validation_length]\n",
    "y_train = moons[1][:validation_length]\n",
    "X_validate = moons[0][validation_length:]\n",
    "y_validate = moons[1][validation_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('poly_features', PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=3)), # convert to polynomial. Increase the degree to fit better (possibly)\n",
    "    (\"scaler\", StandardScaler()), # standardize the data\n",
    "    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "    # C defines the tradeoff between hard and soft margins\n",
    "])\n",
    "\n",
    "polynomial_svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = polynomial_svm_clf.predict(X_validate)\n",
    "accuracy_score = np.mean(y_pred == y_validate)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear SVMs using the SVC class with Polynomial kernel\n",
    "(preferred b/c SVC knows to use the kernel trick to save space)  \n",
    "  \n",
    "For an instance x_i, it will compute (x_i)^2, ..., (x_i)^n for a polynomial of degree n  \n",
    "These new values will be used as **new features = adding more dimensions in hopes that the data will be more linearly separable.**  \n",
    "  \n",
    "When using the kernel trick, it doesn't have to store each newly computed feature. It can just compute as it goes along based on some mathematical transformation properties not outlined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=5, cache_size=200, class_weight=None, coef0=1,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5)) \n",
    "    # coef=1 scales the data, C defines the tradeoff between hard and soft margins\n",
    "])\n",
    "\n",
    "poly_kernel_svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = polynomial_svm_clf.predict(X_validate)\n",
    "accuracy_score = np.mean(y_pred == y_validate)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear SVMs using the SVC class with Gaussian RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall:  \n",
    "The Gaussian RBF kernel randomly picks N points (corresponds to degree=N)  \n",
    "**For each point, it calculates the distance of all data points from that point (aka how similiar each piece of data is to that point) through a normal-esque distribution**  \n",
    "Then, you have N new features, and you can use this N-dimensional data to try and linearly separate the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=5, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", degree=3, gamma=5, C=0.001))\n",
    "    # gamma defines how narrow the normal distribution will be, C defines the tradeoff between hard and soft margins\n",
    "])\n",
    "\n",
    "poly_kernel_svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = polynomial_svm_clf.predict(X_validate)\n",
    "accuracy_score = np.mean(y_pred == y_validate)\n",
    "accuracy_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Batch GD with Early Stopping for Softmax Regression by Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(2042)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = (2, 3)\n",
    "classlen = len(iris[\"target_names\"])\n",
    "X = iris[\"data\"][:, features]\n",
    "y = iris[\"target\"]\n",
    "X_w_bias = np.c_[np.ones([len(X), 1]), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(vec):\n",
    "    return np.sum(np.exp(vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move around and separate data to training, testing, and validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2\n",
    "total_size = len(X_w_bias)\n",
    "\n",
    "# Determine how data should be distributed\n",
    "test_size = int(total_size*test_ratio)\n",
    "validation_size = int(total_size*validation_ratio)\n",
    "train_size = total_size - test_size - validation_size\n",
    "\n",
    "rnd_indices = np.random.permutation(total_size) #randomly rearrange training data\n",
    "\n",
    "# Split data by its ratios\n",
    "X_train = X_w_bias[rnd_indices[:train_size]]\n",
    "y_train = y[rnd_indices[:train_size]]\n",
    "X_test = X_w_bias[rnd_indices[train_size:-test_size]] #exclude test_size amt of data starting from the end\n",
    "y_test = y[rnd_indices[train_size:-test_size]]\n",
    "X_valid = X_w_bias[rnd_indices[-test_size:]]\n",
    "y_valid = y[rnd_indices[-test_size:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot boys for the easy life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 1, 1, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create one-hot vector out of existing vector\n",
    "def to_one_hot(vec):\n",
    "    n_classes = vec.max() + 1 # max value in vector is 2, meaning 3 classes (since it was originally 0-indexed)\n",
    "    m = len(vec)\n",
    "    Y_one_hot = np.zeros((m, n_classes))\n",
    "    Y_one_hot[np.arange(m), vec] = 1 \n",
    "    # np.arange(m) returns evenly spaced values b/w 0 to m, so it will just output 0,1,...,m\n",
    "    # y is the index of the correct class, so at Y_one_hot[i, correct_class_Ind] it will be 1\n",
    "    return Y_one_hot\n",
    "\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_one_hot(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = to_one_hot(y_train)\n",
    "y_test_one_hot = to_one_hot(y_test)\n",
    "y_valid_one_hot = to_one_hot(y_valid)\n",
    "\n",
    "def softmax(vec):\n",
    "    return np.exp(vec) / np.sum(np.exp(vec), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1] # == 3 b/c 2 inputs and bias term\n",
    "n_outputs = classlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a plain softmax model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.103801941146351\n",
      "1000 0.6619775377544007\n",
      "2000 0.5284247416017703\n",
      "3000 0.46205628837815627\n",
      "4000 0.4202882767705059\n",
      "5000 0.3903164138905715\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01\n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7 #in case gradient hits x = 0 is not differentiable, we don't want errors\n",
    "\n",
    "theta = np.random.rand(n_inputs, n_outputs) # random theta weights matrix. [(features + bias) x classes]\n",
    "\n",
    "for it in range(n_iterations):\n",
    "    logits = X_train.dot(theta) # [insts x (features + bias)] * [(features + bias) x class_probs] = [insts x class_probs]\n",
    "    Y_proba = softmax(logits) # turn into probabilities per instance\n",
    "    loss = -np.mean(np.sum(y_train_one_hot*np.log(Y_proba + epsilon), axis=1)) # cross entropy loss function\n",
    "    error = Y_proba - y_train_one_hot\n",
    "    if it % 1000 == 0:\n",
    "        print(it, loss)\n",
    "    grad = 1/m * X_train.T.dot(error) # 1/m * (X^T * (P.k - Y.k))\n",
    "    # X_train.T = [(features + bias) x inst], error = [inst x class_probs]\n",
    "    theta -= eta*grad\n",
    "\n",
    "    # Won't get much lower than 0.39, if you print error instead you will see how small the error is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.52086430e+00, -1.81208371e-01, -2.48681789e+00],\n",
       "       [ 1.00928135e-01,  1.08123288e+00,  9.24694983e-01],\n",
       "       [-1.38699073e+00, -3.46989769e-03,  1.97108268e+00]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta # trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid.dot(theta)\n",
    "y_proba = softmax(logits)\n",
    "y_predict = np.argmax(y_proba, axis=1) # get the index of the \"1\" value in the one-hot subvectors in the matrix\n",
    "\n",
    "accuracy_score = np.mean(y_valid == y_predict) # takes the mean of a boolean vector (0 = wrong, 1 = correct)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a softmax model with some regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0432300272914143\n",
      "1000 0.6858662577216126\n",
      "2000 0.598785478354788\n",
      "3000 0.562467148871917\n",
      "4000 0.5425341016720427\n",
      "5000 0.5298213143367644\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01\n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7 # in case gradient hits x = 0 is not differentiable, we don't want errors\n",
    "alpha = 0.1 # regularization hyperparameter\n",
    "\n",
    "theta = np.random.rand(n_inputs, n_outputs) # random theta weights matrix. [(features + bias) x classes]\n",
    "\n",
    "for it in range(n_iterations):\n",
    "    logits = X_train.dot(theta) # [insts x (features + bias)] * [(features + bias) x class_probs] = [insts x class_probs]\n",
    "    Y_proba = softmax(logits) # turn into probabilities per instance\n",
    "    cross_entropy_loss = -np.mean(np.sum(y_train_one_hot*np.log(Y_proba + epsilon), axis=1)) # cross entropy loss function\n",
    "    ridge_loss = 1/2 * np.sum(theta[1:] ** 2) # sums the squares of all weights in the input arr\n",
    "    \n",
    "    # ( if it had been specified axis=1, then it would've summed along the rows. similarly, axis=2 sums along columns)\n",
    "    \n",
    "    loss = cross_entropy_loss + alpha * ridge_loss\n",
    "    error = Y_proba - y_train_one_hot\n",
    "    if it % 1000 == 0:\n",
    "        print(it, loss)\n",
    "    grad = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * theta[1:]] \n",
    "    # 1/m * (X^T * (P.k - Y.k)) + regularization\n",
    "    # X_train.T = [(features + bias) x inst], error = [inst x class_probs]\n",
    "    # bias gradients are 0, appended as column 0 for all instances\n",
    "    theta -= eta*grad\n",
    "\n",
    "    # Loss seems greater than before, but perhaps it will do better w/ less overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid.dot(theta)\n",
    "y_proba = softmax(logits)\n",
    "y_predict = np.argmax(y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization + Early Stopping Softmax Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.7794207116643639\n",
      "1000 0.7006165328110734\n",
      "2000 0.6027401729308076\n",
      "3000 0.5639843447895967\n",
      "4000 0.5432236134258799\n",
      "5000 0.5301718706474263\n",
      "6000 0.5211868720693656\n",
      "7000 0.5146426003445033\n",
      "8000 0.5096957217453889\n",
      "9000 0.5058599537861246\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01\n",
    "n_iterations = 10000\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7 # in case gradient hits x = 0 is not differentiable, we don't want errors\n",
    "alpha = 0.1 # regularization hyperparameter\n",
    "overfitLimit = 5\n",
    "overfitCount = 0 # allow overfitLimit amt of times for error to increase before stopping model\n",
    "minLoss = 0 # track previous loss amt\n",
    "\n",
    "theta = np.random.rand(n_inputs, n_outputs) # random theta weights matrix. [(features + bias) x classes]\n",
    "\n",
    "for it in range(n_iterations):\n",
    "    logits = X_train.dot(theta) # [insts x (features + bias)] * [(features + bias) x class_probs] = [insts x class_probs]\n",
    "    Y_proba = softmax(logits) # turn into probabilities per instance\n",
    "    cross_entropy_loss = -np.mean(np.sum(y_train_one_hot*np.log(Y_proba + epsilon), axis=1)) # cross entropy loss function\n",
    "    ridge_loss = 1/2 * np.sum(theta[1:] ** 2) # sums the squares of all weights in the input arr\n",
    "    \n",
    "    # ( if it had been specified axis=1, then it would've summed along the rows. similarly, axis=2 sums along columns)\n",
    "\n",
    "    loss = cross_entropy_loss + alpha * ridge_loss\n",
    "    error = Y_proba - y_train_one_hot\n",
    "    if it % 1000 == 0:\n",
    "        print(it, loss)\n",
    "    \n",
    "    if it > 1 and minLoss < loss:\n",
    "        print(\"final iteration:\", str(it))\n",
    "        break\n",
    "    else:\n",
    "        minLoss = loss\n",
    "        overfitCount = 0\n",
    "    \n",
    "    grad = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * theta[1:]] \n",
    "    # 1/m * (X^T * (P.k - Y.k)) + regularization\n",
    "    # X_train.T = [(features + bias) x inst], error = [inst x class_probs]\n",
    "    # bias gradients are 0, appended as column 0 for all instances\n",
    "    theta -= eta*grad\n",
    "\n",
    "    # For some reason error doesn't go up = no overfitting?? However, this early stopping should conceptually work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid.dot(theta)\n",
    "y_proba = softmax(logits)\n",
    "y_predict = np.argmax(y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
